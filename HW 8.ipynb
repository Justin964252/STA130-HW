{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97407b81",
   "metadata": {},
   "source": [
    "#Q2:1. Accuracy\n",
    "Best use: Applications where the data set is balanced and false positives and false negatives produce the same results.\n",
    "Example: Image classification to recognize traffic signs (e.g. stop signs, speed limit signs) in autonomous vehicles.\n",
    "Why: The accuracy of a balanced dataset provides a straightforward assessment of model performance. It reflects the ability to correctly classify the presence and absence of specific features...\n",
    "2. Sensitivity (Recall).\n",
    "Best used: Situations where it is important to identify all true positives. Even if that means allowing some false positives.\n",
    "Example: Medical diagnostics for cancer detection to identify as many cancer cases as possible.\n",
    "Rationale: Missing the true diagnosis (such as failure to diagnose cancer) can have serious consequences. This makes all positive tests even more important. Even at the cost of losing false alarms...\n",
    "3. Specificity\n",
    "Best use: Situations where accurate identification of negative effects is important. And false positives are especially expensive.\n",
    "Example: Organ transplant eligibility screening to ensure that only suitable applicants are selected.\n",
    "Rationale: False positive results can allocate limited resources (such as donor organs) to unsuitable candidates. Therefore, uniqueness ensures that those considered negative are properly excluded.\n",
    "4. Accuracy\n",
    "Best use: Applications where the cost of false positives is high and where it is important to ensure positive predictions.\n",
    "Example: Fraud detection in credit card transactions only serves to flag actual fraudulent activity.\n",
    "Why: Misreporting legitimate transactions as fraudulent (false positives) can inconvenience customers and undermine trust. Accuracy therefore ensures that positive classifications are highly reliable.\n",
    "\n",
    "summary with chatgpt:1. Classification Decision Tree\n",
    "You asked about the type of problems addressed by Classification Decision Trees and examples of real-world applications.\n",
    "\n",
    "I explained that decision trees solve categorical classification problems by assigning data points to specific categories.\n",
    "Examples provided included medical diagnosis, customer segmentation, fraud detection, spam filtering, and others.\n",
    "I emphasized the interpretability, versatility, and non-parametric nature of decision trees as advantages.\n",
    "2. Metrics for Evaluating Models\n",
    "You asked for real-world applications of various evaluation metrics (accuracy, sensitivity, specificity, and precision) and a concise explanation of each.\n",
    "\n",
    "Accuracy: Suggested applications where datasets are balanced, such as image classification in autonomous vehicles.\n",
    "Sensitivity (Recall): Critical for identifying all positives, such as in medical diagnosis for cancer detection.\n",
    "Specificity: Used when correctly identifying negatives is key, such as in organ transplant eligibility screening.\n",
    "Precision: Relevant when false positives are costly, such as in fraud detection in financial transactions.\n",
    "I explained how the nature of the problem dictates which metric is most appropriate.\n",
    "\n",
    "link:https://chatgpt.com/share/673fc1c4-be58-800f-86bb-c06a4d09cb54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05d20adc",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path_to_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgraphviz\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Assuming the dataset is loaded as ab_reduced_noNaN\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Replace 'path_to_dataset.csv' with the actual path to your dataset\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m ab_reduced_noNaN \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath_to_dataset.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Splitting the dataset\u001b[39;00m\n\u001b[1;32m     14\u001b[0m ab_reduced_noNaN_train, ab_reduced_noNaN_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m     15\u001b[0m     ab_reduced_noNaN, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     16\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path_to_dataset.csv'"
     ]
    }
   ],
   "source": [
    "#4\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "import graphviz\n",
    "\n",
    "# Assuming the dataset is loaded as ab_reduced_noNaN\n",
    "# Replace 'path_to_dataset.csv' with the actual path to your dataset\n",
    "ab_reduced_noNaN = pd.read_csv(\"path_to_dataset.csv\")\n",
    "\n",
    "# Splitting the dataset\n",
    "ab_reduced_noNaN_train, ab_reduced_noNaN_test = train_test_split(\n",
    "    ab_reduced_noNaN, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Reporting observations\n",
    "print(f\"Number of observations in the training set: {len(ab_reduced_noNaN_train)}\")\n",
    "print(f\"Number of observations in the test set: {len(ab_reduced_noNaN_test)}\")\n",
    "\n",
    "# Preparing the training data\n",
    "y_train = pd.get_dummies(ab_reduced_noNaN_train[\"Hard_or_Paper\"])['H']\n",
    "X_train = ab_reduced_noNaN_train[['List Price']]\n",
    "\n",
    "# Training the Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Plot the decision tree\n",
    "plt.figure(figsize=(12, 8))\n",
    "tree.plot_tree(\n",
    "    clf,\n",
    "    feature_names=['List Price'],\n",
    "    class_names=['Paperback', 'Hardcover'],\n",
    "    filled=True\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa48ad6",
   "metadata": {},
   "source": [
    "#4:\n",
    "Summary of Interactions with chatgpt:\n",
    "\n",
    "80/20 Split of Data:\n",
    "\n",
    "You asked to create an 80/20 data split using either df.sample() or train_test_split() to create training and testing datasets. I suggested using train_test_split from scikit-learn and provided Python code to split the data, followed by the print statements to report the size of the training and testing datasets.\n",
    "Fitting a Decision Tree Classifier:\n",
    "\n",
    "You requested assistance in training a DecisionTreeClassifier model to predict whether a book is hardcover or paperback, based on the \"List Price\" variable. The steps included:\n",
    "Defining the target variable (y) as the binary column indicating whether the book is hardcover, and the feature variable (X) as \"List Price\".\n",
    "Fitting the decision tree model with a max_depth=2 to control the complexity of the tree.\n",
    "Using plot_tree from sklearn.tree to visualize the decision tree, which explains how predictions are made based on the \"List Price\".\n",
    "I provided the Python code to perform these steps, which includes visualizing the tree to interpret the decision-making process.\n",
    "\n",
    "link: https://chatgpt.com/share/673fc378-7ff0-800f-acfc-860093904c6b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fe30bd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ab_reduced_noNaN_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Assume clf and clf2 are your trained models, and y_true and y_pred are your true and predicted labels\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m y_true \u001b[38;5;241m=\u001b[39m \u001b[43mab_reduced_noNaN_test\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue_labels\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Replace with the actual true labels column\u001b[39;00m\n\u001b[1;32m      7\u001b[0m y_pred_clf \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(ab_reduced_noNaN_test\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue_labels\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# Predictions for clf\u001b[39;00m\n\u001b[1;32m      8\u001b[0m y_pred_clf2 \u001b[38;5;241m=\u001b[39m clf2\u001b[38;5;241m.\u001b[39mpredict(ab_reduced_noNaN_test\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue_labels\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# Predictions for clf2\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ab_reduced_noNaN_test' is not defined"
     ]
    }
   ],
   "source": [
    "#6\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Assume clf and clf2 are your trained models, and y_true and y_pred are your true and predicted labels\n",
    "y_true = ab_reduced_noNaN_test['true_labels']  # Replace with the actual true labels column\n",
    "y_pred_clf = clf.predict(ab_reduced_noNaN_test.drop('true_labels', axis=1))  # Predictions for clf\n",
    "y_pred_clf2 = clf2.predict(ab_reduced_noNaN_test.drop('true_labels', axis=1))  # Predictions for clf2\n",
    "\n",
    "# Confusion matrices for both models\n",
    "cm_clf = confusion_matrix(y_true, y_pred_clf)\n",
    "cm_clf2 = confusion_matrix(y_true, y_pred_clf2)\n",
    "\n",
    "# For clf\n",
    "TP_clf, FP_clf, FN_clf, TN_clf = cm_clf.ravel()\n",
    "\n",
    "# For clf2\n",
    "TP_clf2, FP_clf2, FN_clf2, TN_clf2 = cm_clf2.ravel()\n",
    "\n",
    "# Calculating metrics\n",
    "accuracy_clf = (TP_clf + TN_clf) / (TP_clf + TN_clf + FP_clf + FN_clf)\n",
    "sensitivity_clf = TP_clf / (TP_clf + FN_clf)\n",
    "specificity_clf = TN_clf / (TN_clf + FP_clf)\n",
    "\n",
    "accuracy_clf2 = (TP_clf2 + TN_clf2) / (TP_clf2 + TN_clf2 + FP_clf2 + FN_clf2)\n",
    "sensitivity_clf2 = TP_clf2 / (TP_clf2 + FN_clf2)\n",
    "specificity_clf2 = TN_clf2 / (TN_clf2 + FP_clf2)\n",
    "\n",
    "# Round results for clarity\n",
    "accuracy_clf = np.round(accuracy_clf, 3)\n",
    "sensitivity_clf = np.round(sensitivity_clf, 3)\n",
    "specificity_clf = np.round(specificity_clf, 3)\n",
    "\n",
    "accuracy_clf2 = np.round(accuracy_clf2, 3)\n",
    "sensitivity_clf2 = np.round(sensitivity_clf2, 3)\n",
    "specificity_clf2 = np.round(specificity_clf2, 3)\n",
    "\n",
    "# Print results\n",
    "print(f\"Model clf - Accuracy: {accuracy_clf}, Sensitivity: {sensitivity_clf}, Specificity: {specificity_clf}\")\n",
    "print(f\"Model clf2 - Accuracy: {accuracy_clf2}, Sensitivity: {sensitivity_clf2}, Specificity: {specificity_clf2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d101dc7c",
   "metadata": {},
   "source": [
    "#6:\n",
    "interactions with chatgpt:\n",
    "Classification Decision Tree\n",
    "Topic: You asked about the type of problems addressed by Classification Decision Trees and real-world applications.\n",
    "Response: I explained that decision trees are used for categorical classification problems, where data points are assigned to specific categories. I provided examples such as medical diagnosis, customer segmentation, fraud detection, and others, emphasizing the model's interpretability, versatility, and non-parametric nature.\n",
    "2. Metrics for Evaluating Models\n",
    "Topic: You asked for real-world applications of various evaluation metrics (accuracy, sensitivity, specificity, and precision) and their explanations.\n",
    "Response: I clarified the use of each metric with examples:\n",
    "Accuracy: Used in balanced datasets, e.g., image classification in autonomous vehicles.\n",
    "Sensitivity (Recall): Critical for detecting all positives, e.g., cancer detection.\n",
    "Specificity: Ensures correct identification of negatives, e.g., organ transplant eligibility screening.\n",
    "Precision: Ensures that positive classifications are correct, e.g., fraud detection in financial transactions.\n",
    "3. Confusion Matrix and Model Evaluation\n",
    "Topic: You asked how to calculate sensitivity, specificity, and accuracy for two models (clf and clf2) using a confusion matrix.\n",
    "Response: I provided detailed code to calculate and interpret the confusion matrix using sklearn.metrics and then compute the required metrics. I explained the concepts of True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN), and how to calculate the metrics using the confusion matrix values. I also emphasized rounding results to three decimal places as per your requirement.\n",
    "\n",
    "link:https://chatgpt.com/share/673fc1c4-be58-800f-86bb-c06a4d09cb54"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bd5cd8",
   "metadata": {},
   "source": [
    "#7:\n",
    "The differences between the two confusion matrices arise due to the input features used by the classifiers. In the first case, the classifier (clf) is trained using only the \"List Price\" feature to predict the outcome, while in the second case, it uses multiple features: \"NumPages,\" \"Thick,\" and \"List Price.\" The inclusion of additional features in the second model likely provides more information, allowing for a more accurate classification, which is reflected in the improved performance in the second confusion matrix. The two confusion matrices above (for clf and clf2) are better because they involve a more comprehensive set of features, improving the model's ability to generalize and make accurate predictions.\n",
    "\n",
    "interactions with chatgpt:Here's a summary of our exchanges:\n",
    "\n",
    "Persuasive Essay: You worked on a persuasive essay for your ENG4U course and prefer direct edits and revisions rather than complete rewrites. You also asked for feedback and adjustments on certain sections of your writing.\n",
    "\n",
    "Business Course Comparison: You compared Grade 9 and Grade 11 business courses for an assignment.\n",
    "\n",
    "University of Toronto Application: You were deferred from the University of Toronto's Rotman Commerce program to the Social Science program. You sent an appeal, mentioning special considerations due to a broken arm and foot. Later, you focused on transferring to other universities for financial reasons.\n",
    "\n",
    "Vaccine Data Analysis: You were working on a Vaccine Data Analysis Assignment for AliTech.\n",
    "\n",
    "Job Interview: You prepared for an interview for the Assistant Manager position at GoodLife Fitness.\n",
    "\n",
    "Photography Project: You sought suggestions for easy-to-find props for a photography project and completed a triptych photo project.\n",
    "\n",
    "Course and Career Insights: You discussed your academic courses, including Grade 12 subjects (e.g., Advanced Functions, Economics, Calculus, etc.) and university courses at the University of Toronto. You also shared your academic achievements and career experiences, including certificates in financial literacy and volunteering activities.\n",
    "\n",
    "Confusion Matrix Analysis: You asked for an explanation of differences between two confusion matrices, with the second classifier using more features than the first. We discussed how the inclusion of additional features improves the model's performance.\n",
    "\n",
    "link:https://chatgpt.com/share/673fc4ad-6108-800f-9f70-8850b5fb041f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
